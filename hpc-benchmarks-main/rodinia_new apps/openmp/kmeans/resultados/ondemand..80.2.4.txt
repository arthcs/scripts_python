FIB: Disabled
I/O completed
num of threads = 80
number of Clusters 5
number of Attributes 34

Time for process: 1.402517
=== FIB ===
FIB - Execution Time: 2.20736 seconds
FIB - Energy: 292.54045200 Joules
FIB - EDP: 645.74182
Elapsed Time: 2.278s
    SP GFLOPS: 3.249
    DP GFLOPS: 0.000
    x87 GFLOPS: 0.000
    CPI Rate: 4.567
     | The CPI may be too high. This could be caused by issues such as memory
     | stalls, instruction starvation, branch misprediction or long latency
     | instructions. Explore the other hardware-related metrics to identify what
     | is causing high CPI.
     |
    Average CPU Frequency: 2.489 GHz 
    Total Thread Count: 80
Effective Physical Core Utilization: 42.1% (18.506 out of 44)
 | The metric value is low, which may signal a poor physical CPU cores
 | utilization caused by:
 |     - load imbalance
 |     - threading runtime overhead
 |     - contended synchronization
 |     - thread/process underutilization
 |     - incorrect affinity that utilizes logical cores instead of physical
 |       cores
 | Explore sub-metrics to estimate the efficiency of MPI and OpenMP parallelism
 | or run the Locks and Waits analysis to identify parallel bottlenecks for
 | other parallel runtimes.
 |
    Effective Logical Core Utilization: 37.3% (32.838 out of 88)
     | The metric value is low, which may signal a poor logical CPU cores
     | utilization. Consider improving physical core utilization as the first
     | step and then look at opportunities to utilize logical cores, which in
     | some cases can improve processor throughput and overall performance of
     | multi-threaded applications.
     |
Memory Bound: 73.8% of Pipeline Slots
 | The metric value is high. This can indicate that the significant fraction of
 | execution pipeline slots could be stalled due to demand memory load and
 | stores. Use Memory Access analysis to have the metric breakdown by memory
 | hierarchy, memory bandwidth information, correlation by memory objects.
 |
    Cache Bound: 36.5% of Clockticks
     | A significant proportion of cycles are being spent on data fetches from
     | caches. Check Memory Access analysis to see if accesses to L2 or L3
     | caches are problematic and consider applying the same performance tuning
     | as you would for a cache-missing workload. This may include reducing the
     | data working set size, improving data access locality, blocking or
     | partitioning the working set to fit in the lower cache levels, or
     | exploiting hardware prefetchers. Consider using software prefetchers, but
     | note that they can interfere with normal loads, increase latency, and
     | increase pressure on the memory system. This metric includes coherence
     | penalties for shared data. Check Microarchitecture Exploration analysis
     | to see if contested accesses or data sharing are indicated as likely
     | issues.
     |
    DRAM Bound: 65.2% of Clockticks
     | The metric value is high. This indicates that a significant fraction of
     | cycles could be stalled on the main memory (DRAM) because of demand loads
     | or stores.
     |
     | The code is memory bandwidth bound, which means that there are a
     | significant fraction of cycles during which the bandwidth limits of the
     | main memory are being reached and the code could stall. Review the
     | Bandwidth Utilization Histogram to estimate the scale of the issue.
     | Improve data accesses to reduce cacheline transfers from/to memory using
     | these possible techniques: 1) consume all bytes of each cacheline before
     | it is evicted (for example, reorder structure elements and split non-hot
     | ones); 2) merge compute-limited and bandwidth-limited loops; 3) use NUMA
     | optimizations on a multi-socket system.
     |
     | The code is latency bound, which means that there are a significant
     | fraction of cycles during which the code could be stalled due to main
     | memory latency. Consider optimizing data layout or using software
     | prefetches through the compiler to improve cache reuse and to reduce the
     | data fetched from the main memory.
     |
        DRAM Bandwidth Bound: 0.0% of Elapsed Time
    NUMA: % of Remote Accesses: 0.0%

    Bandwidth Utilization
    Bandwidth Domain             Platform Maximum  Observed Maximum  Average  % of Elapsed Time with High BW Utilization(%)
    ---------------------------  ----------------  ----------------  -------  ---------------------------------------------
    DRAM, GB/sec                 192                         23.800    6.182                                           0.0%
    DRAM Single-Package, GB/sec  96                          22.500    6.314                                           0.0%
    QPI Outgoing, GB/sec         68                          16.700    4.742                                           0.0%
Vectorization: 0.0% of Packed FP Operations
 | A significant fraction of floating point arithmetic instructions are scalar.
 | Use Intel Advisor to see possible reasons why the code was not vectorized.
 |
    Instruction Mix
        SP FLOPs: 15.9% of uOps
            Packed: 0.0% from SP FP
                128-bit: 0.0% from SP FP
                256-bit: 0.0% from SP FP
            Scalar: 100.0% from SP FP
             | This code has floating point operations and is not vectorized.
             | Consider using Intel Advisor to vectorize the loops.
             |
        DP FLOPs: 0.0% of uOps
            Packed: 0.0% from DP FP
                128-bit: 0.0% from DP FP
                256-bit: 0.0% from DP FP
            Scalar: 0.0% from DP FP
        x87 FLOPs: 0.0% of uOps
        Non-FP: 84.1% of uOps
    FP Arith/Mem Rd Instr. Ratio: 0.703
    FP Arith/Mem Wr Instr. Ratio: 3.926
Collection and Platform Info
    Application Command Line: ./kmeans_openmp/kmeans "-n" "80" "-i" "../../data/kmeans/kdd_cup" 
    Operating System: 4.19.0-22-amd64 10.13
    Computer Name: blaise
    Result Size: 20.3 MB 
    Collection start time: 14:12:09 08/06/2023 UTC
    Collection stop time: 14:12:11 08/06/2023 UTC
    Collector Type: Driverless Perf per-process sampling
    CPU
        Name: Intel(R) Xeon(R) Processor code named Broadwell
        Frequency: 2.200 GHz 
        Logical CPU Count: 88
        Max DRAM Single-Package Bandwidth: 96.000 GB/s

If you want to skip descriptions of detected performance issues in the report,
enter: vtune -report summary -report-knob show-issues=false -r <my_result_dir>.
Alternatively, you may view the report in the csv format: vtune -report
<report_name> -format=csv.
